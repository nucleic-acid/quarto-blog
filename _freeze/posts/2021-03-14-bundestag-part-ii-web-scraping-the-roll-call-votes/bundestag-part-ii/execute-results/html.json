{
  "hash": "a70d9f3c19504190fa083877b7fb3876",
  "result": {
    "markdown": "---\ntitle: \"Bundestag Part II - Web scraping parliamentary votes\"\nsubtitle: |\n  The data on roll-call votes is accessible for the public on the bundestag.de website, but not via an API. Web Scraping to the rescue! In this part I will collect the available data and compose a single, full dataset as preparation for the following posts.\ndate: 2021-03-14\ndraft: true\ncategories:\n  - python\n  - germany\n  - politics\n  - bundestag\n  - web scraping\n  - RegEx\n  - superseded\nimage: images/harvest.jpg\nimage-alt: \"A green harvester driving across a field of wheat.\"\nengine: knitr\njupyter: python3\n---\n\n\n\n:::{.callout-note collapse=true appearance='default' icon=true}\n## Updates\n2022-09-09\n: Ported to quarto\n:::\n\n## Introduction\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nknitr::opts_chunk$set(echo = FALSE, \n                      collapse = TRUE,\n                      comment = \"#>\",\n                      fig.retina = 2, # Control using dpi\n                      fig.width = 6,  # generated images\n                      fig.align = \"center\",\n                      dpi = 72, \n                      out.width = \"100%\",\n                      dev = \"svg\",\n                      dev.args = list(png = list(type = \"cairo-png\")),\n                      optipng = \"-o1 -quiet\")\n# source(\"../../resources/jolly_theme.R\", local = knitr::knit_global())\nlibrary(\"reticulate\")\n# use_condaenv(condaenv = \"r-reticulate\")\n# library(\"r2symbols\")\n```\n:::\n\n\n\n:::{.callout-important collapse=false appearance='default' icon=true}\n## Superseded\nI found a far better way to obtain all of the available ~700 datasets. The process can be found in [Part II.1](https://jollydata.blog/posts/2021-03-22-bundestag-part-ii1/bundestag-part-ii1.html) of this series. I will continue the series with the data obtained in that improved post, but will leave this version online for educational reasons, though.\n:::\n\nThis is the second part of a series on the German national parliament (aka \"Bundestag\"). In the [first part](https://jollydata.blog/posts/2021-03-07-bundestag-part-i/composition-of-the-german-parliament-since-1949.html) I looked at the historical data on the parliamentarians. In further posts of this series I want to take a look at the voting behaviour of the parlamentarians, at least at those votes where the names are registered publicly. For this I need to collect this polling data, which will be this blog post.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![List of polls in the Bundestag (https://www.bundestag.de/parlament/plenum/abstimmung/liste), screenshot taken 14.03.2021.](images/bundestag_liste.png){fig-align='center' width=100%}\n:::\n:::\n\n\n\n\nThe Bundestag [provides polling data](https://www.bundestag.de/parlament/plenum/abstimmung/liste) for quite a long time going back as PDF reports, but only since around 2012 in machine readable tables. They do so on a website that lists the datasets with title of the poll and a download link for PDF and XLS(X) files (s. Figure 1). Unfortunately there is no specific license mentioned for the open data, but the data is offered to \"interested users\" for \"further machine processing\". Sounds good enough for my use.\nAs I couldn't find an API to access this data, I had to fall back to web scraping.\n\n<aside>In this post I will not include any analysis of the data, as the process of data collection is sufficiently long. If you're not interested in this, I invite you to come back for the next part of the series.</aside>\n\n*Preview Image: \"Harvester\", {{< fa copyright >}} Christian A. Gebhard*\n\n## Downloading the data files\n\n### Analysing the Website\nI first tried to use a simple script with only the `requests` package and `beautiful soup`.\nAs this failed (no datasets found in the requested html), I realized, that the list of polls is not static html, but is dynamically created via Java Script, whenever a user opens the page.\n\nTherefore I had to go back and learn about web scraping of dynamic sites. There are several options here. As I'm doing this post in python, I chose [selenium](https://www.selenium.dev), but there are other options such as [Scrapy](https://scrapy.org)\n\nThe `selenium` package provides a \"web driver\" that directs a browser of choice^[Google Chrome in my case]. This way it is possible to perform repetitive tasks on a dynamic page (e.g. clicking \"next page\"), let the scripts load the content of interest and then scrape these from the dynamic html.\n\n:::{.column-page}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Analysing the website of the Bundestag (https://www.bundestag.de/parlament/plenum/abstimmung/liste) with the dev tools, screenshot taken 14.03.2021.](images/dev_tools.png){fig-align='center' width=100%}\n:::\n:::\n\n:::\n\nUsing the dev-tools of my browser I looked up the names of the html objects I needed to access to obtain the data of interest. (s. Figure 2)\n\n\n### Collecting the URLs for the datasets of interest\n\nIn this first section I scraped the (static) download URLs for the Excel files from the dynamic list, as described above. As I commented all major steps in the code chunks below, I will not write redundant details in the text.\n\nFirst all necessary python modules are loaded and some options are set.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.python .cell-code}\n# import selenium's webdriver and set options\nfrom selenium import webdriver\noptions = webdriver.ChromeOptions()\noptions.add_argument('--ignore-certificate-errors')\noptions.add_argument('--incognito')\n\n# Import all other libraries needed for the following steps\nimport requests # for the actual downloads\nimport time # to force pauses in the script\nimport pandas as pd # to handle the collected data\nimport glob # to read all the downloaded files automatically\nimport re # for regular expressions\nfrom datetime import datetime # to handle date variables\n```\n:::\n\n\n\nThe next step is the collection of the download URLs. As seen in Figure 1, there are 73 pages of the list to click through. I hardcoded this number into the script below. However the script got quite slow after ~25 pages and I stopped it, when I realized, that no new URLs were added despite the script still going through the table.\n\n<aside>I didn't do a complete \"post mortem\", why the script got ever slower, but this way I learned that the xls(x) files are only accessible back until October 18th 2012.</aside>\n\nAnother adaption to the script was necessary, as the dynamic html not only contained the table items/rows of the page that was currently viewed, but also all items from the previous pages. This resulted in the first draft of the script cycling over an ever growing number of rows with each new page. In the below version, the last number of rows are skipped, so that only the new items are scraped. \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.python .cell-code}\n# setting up the driver with the above options\ndriver = webdriver.Chrome(\"/usr/lib/chromedriver_dir/chromedriver\", chrome_options=options)\n\n# set a timer for the script to wait until an element to be found, or a command is completed.\ndriver.implicitly_wait(15)\n\n# open the website and wait for 10s to fully load the site\ndriver.get(\"https://www.bundestag.de/parlament/plenum/abstimmung/liste\")\ntime.sleep(10)\n\n# instantiate an empty list to store the URLs\nurl_df = pd.DataFrame(columns = ['name', 'url'])\n\n# instantiate a skipping variable to fix repeating table rows.\nskip_n = 0\n\n# Outer loop to go through all pages of the dynamic table on the website.\n# For now this is hard-coded to 73 pages as seen on the website (s. Figure 1)\nfor i in range(73):\n  \n  # print progress\n  print(\"Working on page \" + str(i + 1) + \":\")\n  \n  # find all target elements (name and urls of files) from the dynamic list\n  datasets = driver.find_elements_by_class_name(\"bt-documents-description\")\n\n  # if the website is empty or wasn't read correctly, stop the loop.\n  if len(datasets) == 0:\n    print(\"No dataset links found on this page - Stopping.\")\n    break\n  \n  # otherwise continue the scraping\n  else:\n    print(str(len(datasets)) + \" datasets found, skipping \" + str(skip_n) + \", - processing...\")\n    \n    # inner loop to go through all elements from the current page\n    for j in range(skip_n, len(datasets)):\n      \n      # extract the title only (i.e. the bold text, not the link texts for the urls)\n      item_name = datasets[j].find_element_by_tag_name(\"strong\").text\n      if item_name == \"\":\n        print(\"Skipping empty item.\")\n        continue\n      \n      # retrieve sub-elements (document urls) from the item\n      elements = datasets[j].find_elements_by_class_name(\"bt-link-dokument\")\n\n      # loop through sub-elements (i.e. pdf and xlsx-links)\n      for elem in elements:\n        \n        # extract the url from the element\n        element_url = elem.get_attribute(\"href\")\n\n        # only store the URL, if it points to an excel sheet\n        if element_url.endswith(\".xlsx\") or element_url.endswith(\".xls\"):\n          \n          item_url = element_url\n          \n          # store name and url in a dict and append to dataframe\n          item_dict = {\"name\" : item_name, \"url\" : item_url}\n          url_df = url_df.append(item_dict, ignore_index = True)\n\n    print(\"This page is complete, scrolling down and going to the next page\")\n    \n    # scroll down the website, since otherwise the \"share\" button overlays \n    # the \"next-button\" which throws an exception and stops the script\n    try:\n      js = 'window.scrollTo(0, document.documentElement.scrollHeight)'\n      driver.execute_script(js)\n      print(\"Scrolled...\")\n      time.sleep(2) # give time to scoll\n    except:\n      print(\"Scrolling failed\")\n      break\n    \n  \n    # find and click the button to the next page:\n    try:\n      python_button = driver.find_element_by_class_name('slick-next.slick-arrow')\n      python_button.click()\n    except:\n      print(\"Clicking failed\")\n      driver.quit()\n      break\n    finally:\n      # pause for 20 seconds before next loop to avoid jamming the website.\n      print(\"Waiting politely for 20 seconds\")\n      time.sleep(20)\n      \n      # store current length of the datasets table to skip these lines in the next loop.\n      skip_n = len(datasets)\n\n# after all pages are scraped, quit connection\nprint(\"All done, quitting connection\")\ndriver.quit()\n\n# write url_list to file\nurl_df.to_csv(\"url_df.csv\")\n\n```\n:::\n\n\nThe above script not only scrapes the URLs to the Excel files, but also the name and date of the poll. I want to include both in the final dataset.\n\n### Download the datasets\n\nAfter compiling the dataframe with the names and URLs, the following script can go through it and download one file at a time. To prevent jamming of the website I forced some seconds of pauses in each loop of the script.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.python .cell-code}\n# read the previously saved data frame\nworking_df = pd.read_csv(\"url_df.csv\", index_col = 0)\n\n# define the characters to remove from the dataset names later\nspecial_chars = ['.', ':', '(', \")\", \"/\", \",\"]\n\n# iterate over the rows of the dataframe\nfor i, row in working_df.iterrows():\n  \n  # first try to download the file via requests library\n  try:\n    response = requests.get(row['url'])\n    \n  except:\n    print(\"Download of file index \" + str(i) + \" failed, exiting loop.\")\n    break\n  \n  else:\n    # if successfull, determine the filetype of the downloaded file  \n    if row['url'].endswith(\".xlsx\"):\n      file_type = \".xlsx\"\n    else:\n      file_type = \".xls\"\n      \n    # compose the path/filename, remove unwanted characters and add filetype.\n    filename = 'downloaded/' + ''.join(c for c in row['name'] if not c in special_chars) + file_type\n    # replace space with underscores\n    filename = filename.replace(\" \", \"_\")\n\n    # open the output file and write the downloaded content\n    with open(filename, \"wb\") as file:\n      file.write(response.content)\n      \n    print(\"File \" + str(i) + \" downloaded.\")\n    \n    # wait 12s before the net iteration to not jam the website\n    time.sleep(12)\n```\n:::\n\n\nIncluding the intermittant breaks, the download period spanned almost 2 hours.\n\n## Processing and combining the data\n\nI noticed some problems in the scraped date and poll-name data (and thus the filenames), that would propagate through the process and later cause problems. After looking up the few needed bits of information, I manually corrected the names of some files:\n\n\n\n### Assembling the datasets\n\nAfter downloading all files, I concatenated them into one single dataframe to facilitate later analyses. In the same process I added the date and title of the polls (that were intermittently stored in the file names) into the dataframe.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.python .cell-code}\n# find all downloaded Excel files (xls and xlsx)\nfile_list = glob.glob(\"downloaded/*.xls*\")\nprint(\"Files found: \" + len(file_list)\n\n#define file-type pattern\npattern = r'\\.xlsx?$'\n\n# initialize empty df\ndf = pd.DataFrame()\n\n# define vars for process checks\nprocessed = 0\nfailed = 0\n\n# loop over all found files in downloaded/ directory\nfor f in range(len(file_list)):\n  \n  # read the dataset txo a temporary df\n  try:\n    temporary_df = pd.read_excel(file_list[f])\n  except:\n    print(\"not read: \" + file_list[f])\n    failed = failed + 1\n    continue\n  finally:\n    processed = processed +1\n  \n\n  # extract the title of the poll and the date from the filename.\n  # Step 1: remove the folder name\n  step1 = file_list[f].replace(\"downloaded/\", \"\", 1)\n  \n  # Step 2: remove file extension via Regular Expression to catch xlsx and xls\n  step2 = re.sub(pattern, '', step1)\n  # print(step2)\n  \n  # Step 3: split into date and title\n  date, title = step2.split(\"_\", 1)\n\n  # convert date to a datetime type and add to temporary df\n  date = datetime.strptime(date, \"%d%m%Y\")\n  temporary_df['date'] = date\n  \n  # replace underscores from filename to get a proper title of the poll\n  title = title.replace(\"_\", \" \")\n  temporary_df['title'] = title\n  \n  # add temporary dataframe to the cumulated df\n  df = pd.concat([df, temporary_df], axis=0, ignore_index = True)\n  \n  \n# print checks and parts of dataframe for quality check\nprint(\"Processed: \" + str(processed))\nprint(\"Failed: \" + str(failed))\nprint(df.shape)\nprint(df.head())\nprint(df.tail())\n\n# check if number of poll matches the number of files read\ndf.drop_duplicates(subset=['Wahlperiode', 'Sitzungnr', 'Abstimmnr'])\n\n\n# save df to file\ndf.to_csv(\"data_complete.csv\")\n\n```\n:::\n\n\n## Summary and Outlook\n\nThis concludes the web scraping. The resulting dataframe has 316044 rows, containing the data of 477 separate polls with the individual votes of all participating representatives. The votes span a period of three electoral periods (2012 until 2021).\n\nAs there is no license specified for the Open Data of the Bundestag I cannot share the scraped data.\n\nI don't know whether the data is collected by the institution in a standardized way or if typos, abbreviations or other inconsistencies need to be accounted for. In the above process I haven't seen any problems of the sort, but there are > 300k lines, that I haven't checked explicitly. Another aspect is, that names of representatives might have changed over time due to marriage or divorce, titles might been earned (or lost).\n\nThe most robust way would be, to use a representative-ID as unique identifier in the further analysis. These IDs and all official name changes are also available in the open data from the Bundestag. In fact I have used this data in the [first part](https://jollydata.blog/posts/2021-03-07-bundestag-part-i/composition-of-the-german-parliament-since-1949.html) of this series, however, there I chose to only use the current/last known name.\n\nI plan to use \"fuzzy\" (string distance) joining, to unify the representatives in the polling data using the dataset with the historical data on the representatives. This will be the next part of the series, so stay tuned!",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}